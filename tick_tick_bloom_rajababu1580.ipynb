{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent.resolve() / 'Tick_Tick_Bloom\\data\\\\final\\public'\n",
    "print(DATA_DIR)\n",
    "assert DATA_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install odc-stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import odc.stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(DATA_DIR/\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(DATA_DIR/\"train_labels.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.split.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the default geopandas base map file to plot points on\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# map the training data\n",
    "base = world[world.name ==\"United States of America\"].plot(edgecolor = \"gray\",\n",
    "        color = \"ghostwhite\",figsize=(9,4),alpha=0.3,ax=ax)\n",
    "train_meta = metadata[metadata[\"split\"]==\"train\"]\n",
    "geometry = [Point(xy) for xy in zip(train_meta[\"longitude\"],train_meta[\"latitude\"])]\n",
    "gdf = gpd.GeoDataFrame(train_meta,geometry =geometry)\n",
    "gdf.plot(ax=base,marker=\".\",markersize = 3,color=\"blue\",label = \"Train\",alpha=0.6)\n",
    "\n",
    "# map the test data\n",
    "test_meta = metadata[metadata[\"split\"]==\"test\"]\n",
    "geometry = [Point(xy) for xy in zip(test_meta[\"longitude\"],test_meta[\"latitude\"])]\n",
    "gdf = gpd.GeoDataFrame(test_meta,geometry=geometry)\n",
    "gdf.plot(ax=base,marker = \".\",markersize=3,color = \"orange\",label= \"Test\",alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.xlim([-125,-65])\n",
    "plt.ylim([25,50])\n",
    "plt.legend(loc = 4,markerscale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data range are the samples from?\n",
    "# convert data to pd.Datetime\n",
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "\n",
    "# what is the date range?\n",
    "metadata.groupby(\"split\").agg(min_date = (\"date\",min),max_date = (\"date\",max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what years are in the data?\n",
    "pd.crosstab(metadata.date.dt.year,metadata.split).plot(kind=\"bar\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"Distribution of years in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what seasons are the data points from?\n",
    "metadata[\"season\"]=(\n",
    "    metadata.date.dt.month.replace([12,1,2],\"winter\")\n",
    "    .replace([3,4,5],\"spring\")\n",
    "    .replace([6,7,8],\"summer\")\n",
    "    .replace([9,10,11],\"fall\")\n",
    ")\n",
    "metadata.season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is data from for each season?\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "\n",
    "for season, ax in zip(metadata.season.unique(), axes.flatten()):\n",
    "    base = world[world.name == \"United States of America\"].plot(\n",
    "        edgecolor=\"gray\", color=\"ghostwhite\", alpha=0.3, ax=ax\n",
    "    )\n",
    "\n",
    "    sub = metadata[metadata.season == season]\n",
    "    geometry = [Point(xy) for xy in zip(sub[\"longitude\"], sub[\"latitude\"])]\n",
    "    gdf = gpd.GeoDataFrame(sub, geometry=geometry)\n",
    "    gdf.plot(ax=base, marker=\".\", markersize=2.5)\n",
    "    ax.set_xlim([-125, -66])\n",
    "    ax.set_ylim([25, 50])\n",
    "    ax.set_title(f\"{season.capitalize()} data points\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_and_metadata = train_df.merge(\n",
    "    metadata, how=\"left\", left_on=\"uid\", right_on=\"uid\", validate=\"1:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_counts = (\n",
    "    train_df.replace(\n",
    "        {\n",
    "            \"severity\": {\n",
    "                1: \"1 (<20,000)\",\n",
    "                2: \"2 (20,000-100,000)\",\n",
    "                3: \"3 (100,000 - 1,000,000)\",\n",
    "                4: \"4 (1,00,000 - 10,000,000)\",\n",
    "                5: \"5 (>10,000,00)\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    .severity.value_counts()\n",
    "    .sort_index(ascending=False)\n",
    ")\n",
    "plt.barh(severity_counts.index, severity_counts.values)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Severity (range in cells/mL)\")\n",
    "plt.title(\"Train labels severity level counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.density.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df.density == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DATA_DIR/\"submission_format.csv\", index_col=0)\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install planetary-computer\n",
    "%pip install pystac-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the STAC API\n",
    "import planetary_computer as pc\n",
    "from pystac_client import Client\n",
    "\n",
    "catalog = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=pc.sign_inplace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = metadata[metadata.uid == \"garm\"].iloc[0]\n",
    "example_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance as distance\n",
    "# get our bounding box to search latitude and longitude coordinates\n",
    "def get_bounding_box(latitude, longitude, meter_buffer=50000):\n",
    "    \"\"\"\n",
    "    Given a latitude, longitude, and buffer in meters, returns a bounding\n",
    "    box around the point with the buffer on the left, right, top, and bottom.\n",
    "\n",
    "    Returns a list of [minx, miny, maxx, maxy]\n",
    "    \"\"\"\n",
    "    distance_search = distance.distance(meters=meter_buffer)\n",
    "\n",
    "    # calculate the lat/long bounds based on ground distance\n",
    "    # bearings are cardinal directions to move (south, west, north, and east)\n",
    "    min_lat = distance_search.destination((latitude, longitude), bearing=180)[0]\n",
    "    min_long = distance_search.destination((latitude, longitude), bearing=270)[1]\n",
    "    max_lat = distance_search.destination((latitude, longitude), bearing=0)[0]\n",
    "    max_long = distance_search.destination((latitude, longitude), bearing=90)[1]\n",
    "\n",
    "    return [min_long, min_lat, max_long, max_lat]\n",
    "\n",
    "\n",
    "bbox = get_bounding_box(example_row.latitude, example_row.longitude, meter_buffer=50000)\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our date range to search, and format correctly for query\n",
    "def get_date_range(date, time_buffer_days=15):\n",
    "    \"\"\"Get a date range to search for in the planetary computer based\n",
    "    on a sample's date. The time range will include the sample date\n",
    "    and time_buffer_days days prior\n",
    "\n",
    "    Returns a string\"\"\"\n",
    "    datetime_format = \"%Y-%m-%dT\"\n",
    "    range_start = pd.to_datetime(date) - timedelta(days=time_buffer_days)\n",
    "    date_range = f\"{range_start.strftime(datetime_format)}/{pd.to_datetime(date).strftime(datetime_format)}\"\n",
    "\n",
    "    return date_range\n",
    "\n",
    "\n",
    "date_range = get_date_range(example_row.date)\n",
    "date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the planetary computer sentinel-l2a and landsat level-2 collections\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"], bbox=bbox, datetime=date_range\n",
    ")\n",
    "\n",
    "# see how many items were returned\n",
    "items = [item for item in search.get_all_items()]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get details of all of the items returned\n",
    "item_details = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"datetime\": item.datetime.strftime(\"%Y-%m-%d\"),\n",
    "            \"platform\": item.properties[\"platform\"],\n",
    "            \"min_long\": item.bbox[0],\n",
    "            \"max_long\": item.bbox[2],\n",
    "            \"min_lat\": item.bbox[1],\n",
    "            \"max_lat\": item.bbox[3],\n",
    "            \"bbox\": item.bbox,\n",
    "            \"item_obj\": item,\n",
    "        }\n",
    "        for item in items\n",
    "    ]\n",
    ")\n",
    "\n",
    "# check which rows actually contain the sample location\n",
    "item_details[\"contains_sample_point\"] = (\n",
    "    (item_details.min_lat < example_row.latitude)\n",
    "    & (item_details.max_lat > example_row.latitude)\n",
    "    & (item_details.min_long < example_row.longitude)\n",
    "    & (item_details.max_long > example_row.longitude)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Filtering from {len(item_details)} returned to {item_details.contains_sample_point.sum()} items that contain the sample location\"\n",
    ")\n",
    "\n",
    "item_details = item_details[item_details[\"contains_sample_point\"]]\n",
    "item_details[[\"datetime\", \"platform\", \"contains_sample_point\", \"bbox\"]].sort_values(\n",
    "    by=\"datetime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - filter to sentinel\n",
    "item_details[item_details.platform.str.contains(\"Sentinel\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - take closest by date\n",
    "best_item = (\n",
    "    item_details[item_details.platform.str.contains(\"Sentinel\")]\n",
    "    .sort_values(by=\"datetime\", ascending=False)\n",
    "    .iloc[0]\n",
    ")\n",
    "best_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = best_item.item_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What assets are available?\n",
    "for asset_key, asset in item.assets.items():\n",
    "    print(f\"{asset_key:<25} - {asset.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "from IPython.display import Image\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the whole image\n",
    "img = Image(url=item.assets[\"rendered_preview\"].href, width=500)\n",
    "\n",
    "Image(url=item.assets[\"rendered_preview\"].href, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_sentinel_image(item, bounding_box):\n",
    "    \"\"\"\n",
    "    Given a STAC item from Sentinel-2 and a bounding box tuple in the format\n",
    "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
    "    imagery in the bounding box.\n",
    "\n",
    "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
    "    \"\"\"\n",
    "    (minx, miny, maxx, maxy) = bounding_box\n",
    "\n",
    "    image = rioxarray.open_rasterio(pc.sign(item.assets[\"visual\"].href)).rio.clip_box(\n",
    "        minx=minx,\n",
    "        miny=miny,\n",
    "        maxx=maxx,\n",
    "        maxy=maxy,\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    return image.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a smaller geographic bounding box\n",
    "minx, miny, maxx, maxy = get_bounding_box(\n",
    "    example_row.latitude, example_row.longitude, meter_buffer=3000\n",
    ")\n",
    "\n",
    "# get the zoomed in image array\n",
    "bbox = (minx, miny, maxx, maxy)\n",
    "zoomed_img_array = crop_sentinel_image(item, bbox)\n",
    "\n",
    "zoomed_img_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to transpose some of the dimensions to plot\n",
    "# matplotlib expects channels in a certain order\n",
    "plt.imshow(np.transpose(zoomed_img_array, axes=[1, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_item = (\n",
    "    item_details[item_details.platform.str.contains(\"landsat\")]\n",
    "    .sample(n=1, random_state=3)\n",
    "    .iloc[0]\n",
    ")\n",
    "landsat_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_landsat_image(item, bounding_box):\n",
    "    \"\"\"\n",
    "    Given a STAC item from Landsat and a bounding box tuple in the format\n",
    "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
    "    imagery in the bounding box.\n",
    "\n",
    "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
    "    \"\"\"\n",
    "    (minx, miny, maxx, maxy) = bounding_box\n",
    "\n",
    "    image = odc.stac.stac_load(\n",
    "        [pc.sign(item)], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
    "    ).isel(time=0)\n",
    "    image_array = image[[\"red\", \"green\", \"blue\"]].to_array().to_numpy()\n",
    "\n",
    "    # normalize to 0 - 255 values\n",
    "    image_array = cv2.normalize(image_array, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = landsat_item.item_obj\n",
    "\n",
    "# we'll use the same cropped area as above\n",
    "landsat_image_array = crop_landsat_image(item, bbox)\n",
    "landsat_image_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(landsat_image_array, axes=[1, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image but don't convert to numpy or rescale\n",
    "image = odc.stac.stac_load(\n",
    "    [pc.sign(item)], bands=[\"red\", \"green\", \"blue\"], bbox=bbox\n",
    ").isel(time=0)\n",
    "image_array = image[[\"red\", \"green\", \"blue\"]].to_array()\n",
    "\n",
    "# values are not scaled 0 - 255 when first returned\n",
    "image_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image appears differently without rescaling\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "image_array.plot.imshow(robust=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a small area to crop around\n",
    "# crop to 400 meters squared around the sampling point\n",
    "minx, miny, maxx, maxy = get_bounding_box(\n",
    "    example_row.latitude, example_row.longitude, meter_buffer=100\n",
    ")\n",
    "minx, miny, maxx, maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = (minx, miny, maxx, maxy)\n",
    "feature_image_array = crop_sentinel_image(best_item.item_obj, bounding_box=bbox)\n",
    "\n",
    "plt.imshow(np.transpose(feature_image_array, axes=[1, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(feature_image_array), feature_image_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average over the second and third dimensions\n",
    "image_color_averages = feature_image_array.mean(axis=(1, 2)).tolist()\n",
    "\n",
    "# also take the median\n",
    "image_color_medians = np.median(feature_image_array, axis=(1, 2)).tolist()\n",
    "\n",
    "# concatenate the two lists\n",
    "image_features = image_color_averages + image_color_medians\n",
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor our process from above into functions\n",
    "def select_best_item(items, date, latitude, longitude):\n",
    "    \"\"\"\n",
    "    Select the best satellite item given a sample's date, latitude, and longitude.\n",
    "    If any Sentinel-2 imagery is available, returns the closest sentinel-2 image by\n",
    "    time. Otherwise, returns the closest Landsat imagery.\n",
    "\n",
    "    Returns a tuple of (STAC item, item platform name, item date)\n",
    "    \"\"\"\n",
    "    # get item details\n",
    "    item_details = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"datetime\": item.datetime.strftime(\"%Y-%m-%d\"),\n",
    "                \"platform\": item.properties[\"platform\"],\n",
    "                \"min_long\": item.bbox[0],\n",
    "                \"max_long\": item.bbox[2],\n",
    "                \"min_lat\": item.bbox[1],\n",
    "                \"max_lat\": item.bbox[3],\n",
    "                \"item_obj\": item,\n",
    "            }\n",
    "            for item in items\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # filter to items that contain the point location, or return None if none contain the point\n",
    "    item_details[\"contains_sample_point\"] = (\n",
    "        (item_details.min_lat < latitude)\n",
    "        & (item_details.max_lat > latitude)\n",
    "        & (item_details.min_long < longitude)\n",
    "        & (item_details.max_long > longitude)\n",
    "    )\n",
    "    item_details = item_details[item_details[\"contains_sample_point\"] == True]\n",
    "    if len(item_details) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    # add time difference between each item and the sample\n",
    "    item_details[\"time_diff\"] = pd.to_datetime(date) - pd.to_datetime(\n",
    "        item_details[\"datetime\"]\n",
    "    )\n",
    "\n",
    "    # if we have sentinel-2, filter to sentinel-2 images only\n",
    "    item_details[\"sentinel\"] = item_details.platform.str.lower().str.contains(\n",
    "        \"sentinel\"\n",
    "    )\n",
    "    if item_details[\"sentinel\"].any():\n",
    "        item_details = item_details[item_details[\"sentinel\"] == True]\n",
    "\n",
    "    # return the closest imagery by time\n",
    "    best_item = item_details.sort_values(by=\"time_diff\", ascending=True).iloc[0]\n",
    "\n",
    "    return (best_item[\"item_obj\"], best_item[\"platform\"], best_item[\"datetime\"])\n",
    "\n",
    "\n",
    "def image_to_features(image_array):\n",
    "    \"\"\"\n",
    "    Convert an image array of the form (color band, height, width) to a\n",
    "    1-dimensional list of features. Returns a list where the first three\n",
    "    values are the averages of each color band, and the second three\n",
    "    values are the medians of each color band.\n",
    "    \"\"\"\n",
    "    averages = image_array.mean(axis=(1, 2)).tolist()\n",
    "    medians = np.median(image_array, axis=(1, 2)).tolist()\n",
    "\n",
    "    return averages + medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_DATA_DIR = DATA_DIR.parents[1] / \"benchmark\"\n",
    "\n",
    "# save image arrays in case we want to generate more features\n",
    "IMAGE_ARRAY_DIR = BENCHMARK_DATA_DIR / \"image_arrays\"\n",
    "IMAGE_ARRAY_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# take a random subset of the training data for the benchmark\n",
    "train_subset = metadata[metadata[\"split\"] == \"train\"].sample(n=2500, random_state=2)\n",
    "\n",
    "# combine train subset with all test data\n",
    "metadata_subset = pd.concat([train_subset, metadata[metadata[\"split\"] == \"test\"]])\n",
    "metadata_subset.split.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a LONG time because it iterates over all data!\n",
    "\n",
    "# save outputs in dictionaries\n",
    "selected_items = {}\n",
    "features_dict = {}\n",
    "errored_ids = []\n",
    "\n",
    "\n",
    "for row in tqdm(metadata_subset.itertuples(), total=len(metadata_subset)):\n",
    "    pass\n",
    "    # check if we've already saved the selected image array\n",
    "    image_array_pth = IMAGE_ARRAY_DIR / f\"{row.uid}.npy\"\n",
    "\n",
    "    if image_array_pth.exists():\n",
    "        with open(image_array_pth, \"rb\") as f:\n",
    "            image_array = np.load(f)\n",
    "\n",
    "        # convert image to 1-dimensional features\n",
    "        image_features = image_to_features(image_array)\n",
    "        features_dict[row.uid] = image_features\n",
    "\n",
    "    # search and load the image array if not\n",
    "    else:\n",
    "        try:\n",
    "            ## QUERY STAC API\n",
    "            # get query ranges for location and date\n",
    "            search_bbox = get_bounding_box(\n",
    "                row.latitude, row.longitude, meter_buffer=50000\n",
    "            )\n",
    "            date_range = get_date_range(row.date, time_buffer_days=15)\n",
    "\n",
    "            # search the planetary computer\n",
    "            search = catalog.search(\n",
    "                collections=[\"sentinel-2-l2a\", \"landsat-c2-l2\"],\n",
    "                bbox=search_bbox,\n",
    "                datetime=date_range,\n",
    "            )\n",
    "            items = [item for item in search.get_all_items()]\n",
    "\n",
    "            ## GET BEST IMAGE\n",
    "            if len(items) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                best_item, item_platform, item_date = select_best_item(\n",
    "                    items, row.date, row.latitude, row.longitude\n",
    "                )\n",
    "                # add to dictionary tracking best items\n",
    "                selected_items[row.uid] = {\n",
    "                    \"item_object\": best_item,\n",
    "                    \"item_platform\": item_platform,\n",
    "                    \"item_date\": item_date,\n",
    "                }\n",
    "\n",
    "            ## CONVERT TO FEATURES\n",
    "            # get small bbox just for features\n",
    "            feature_bbox = get_bounding_box(\n",
    "                row.latitude, row.longitude, meter_buffer=100\n",
    "            )\n",
    "\n",
    "            # crop the image\n",
    "            if \"sentinel\" in item_platform.lower():\n",
    "                image_array = crop_sentinel_image(best_item, feature_bbox)\n",
    "            else:\n",
    "                image_array = crop_landsat_image(best_item, feature_bbox)\n",
    "\n",
    "            # save image array so we don't have to rerun\n",
    "            with open(image_array_pth, \"wb\") as f:\n",
    "                np.save(f, image_array)\n",
    "\n",
    "            # convert image to 1-dimensional features\n",
    "            image_features = image_to_features(image_array)\n",
    "            features_dict[row.uid] = image_features\n",
    "\n",
    "        # keep track of any that ran into errors without interrupting the process\n",
    "        except:\n",
    "            errored_ids.append(row.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many ran into errors\n",
    "print(f\"Could not pull satellite imagery for {len(errored_ids)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring features into a dataframe\n",
    "image_features = pd.DataFrame(features_dict).T\n",
    "image_features.columns = [\n",
    "    \"red_average\",\n",
    "    \"green_average\",\n",
    "    \"blue_average\",\n",
    "    \"red_median\",\n",
    "    \"green_median\",\n",
    "    \"blue_median\",\n",
    "]\n",
    "image_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out our features!\n",
    "image_features.to_csv(BENCHMARK_DATA_DIR / \"image_features.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring together train labels and features into one dataframe\n",
    "# this ensures the features array and labels array will be in same order\n",
    "train_data = train_df.merge(\n",
    "    image_features, how=\"inner\", left_on=\"uid\", right_index=True, validate=\"1:1\"\n",
    ")\n",
    "\n",
    "# split into train and validation\n",
    "rng = np.random.RandomState(30)\n",
    "train_data[\"split\"] = rng.choice(\n",
    "    [\"train\", \"validation\"], size=len(train_data), replace=True, p=[0.67, 0.33]\n",
    ")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and labels, and train and validation\n",
    "feature_cols = [\n",
    "    \"red_average\",\n",
    "    \"green_average\",\n",
    "    \"blue_average\",\n",
    "    \"red_median\",\n",
    "    \"green_median\",\n",
    "    \"blue_median\",\n",
    "]\n",
    "target_col = \"severity\"\n",
    "\n",
    "val_set_mask = train_data.split == \"validation\"\n",
    "X_train = train_data.loc[~val_set_mask, feature_cols].values\n",
    "y_train = train_data.loc[~val_set_mask, target_col]\n",
    "X_val = train_data.loc[val_set_mask, feature_cols].values\n",
    "y_val = train_data.loc[val_set_mask, target_col]\n",
    "\n",
    "# flatten label data into 1-d arrays\n",
    "y_train = y_train.values.flatten()\n",
    "y_val = y_val.values.flatten()\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see an example of what the data looks like\n",
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"y_train[:10]:\", y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out features\n",
    "x_train_pth = BENCHMARK_DATA_DIR / \"x_train.npy\"\n",
    "x_train_pth.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(x_train_pth, \"wb\") as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "# save out labels\n",
    "y_train_pth = BENCHMARK_DATA_DIR / \"y_train.npy\"\n",
    "\n",
    "with open(y_train_pth, \"wb\") as f:\n",
    "    np.save(f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_gbm_model.py\n",
    "import lightgbm as lgb\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import typer\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data/benchmark\"\n",
    "\n",
    "\n",
    "def main(\n",
    "    features_path=DATA_DIR / \"x_train.npy\",\n",
    "    labels_path=DATA_DIR / \"y_train.npy\",\n",
    "    model_save_path=DATA_DIR / \"lgb_classifier.txt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a LightGBM model based on training features in features_path and\n",
    "    training labels in labels_path. Save our the trained model to model_save_path\n",
    "    \"\"\"\n",
    "\n",
    "    # load saved features and labels\n",
    "    with open(features_path, \"rb\") as f:\n",
    "        X_train = np.load(f)\n",
    "    with open(labels_path, \"rb\") as f:\n",
    "        y_train = np.load(f)\n",
    "\n",
    "    logger.info(f\"Loaded training features of shape {X_train.shape} from {features_path}\")\n",
    "    logger.info(f\"Loading training labels of shape {y_train.shape} from {labels_path}\")\n",
    "\n",
    "    # instantiate tree model\n",
    "    model = lgb.LGBMClassifier(random_state=10)\n",
    "\n",
    "    # fit model\n",
    "    logger.info(\"Fitting LGBM model\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model)\n",
    "\n",
    "    # save out model weights\n",
    "    joblib.dump(model, str(model_save_path))\n",
    "    logger.success(f\"Model weights saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python train_gbm_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out validation features\n",
    "x_val_pth = BENCHMARK_DATA_DIR / \"x_val.npy\"\n",
    "x_val_pth.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(x_val_pth, \"wb\") as f:\n",
    "    np.save(f, X_val)\n",
    "\n",
    "# save out validation labels\n",
    "y_val_pth = BENCHMARK_DATA_DIR / \"y_val.npy\"\n",
    "\n",
    "with open(y_val_pth, \"wb\") as f:\n",
    "    np.save(f, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_gbm_model.py\n",
    "import lightgbm as lgb\n",
    "\n",
    "import joblib\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import typer\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data/benchmark\"\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path=DATA_DIR / \"lgb_classifier.txt\",\n",
    "    features_path=DATA_DIR / \"x_val.npy\",\n",
    "    preds_save_path=DATA_DIR / \"val_preds.npy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions with a LightGBM model using weights saved at model_weights_path\n",
    "    and features saved at features_path. Save out predictions to preds_save_path.\n",
    "    \"\"\"\n",
    "    # load model weights\n",
    "    lgb_model = joblib.load(model_weights_path)\n",
    "    logger.info(f\"Loaded model {lgb_model} from {model_weights_path}\")\n",
    "\n",
    "    # load the features\n",
    "    with open(features_path, \"rb\") as f:\n",
    "        X_val = np.load(f)\n",
    "    logger.info(f\"Loaded features of shape {X_val.shape} from {features_path}\")\n",
    "\n",
    "    # generate predictions\n",
    "    preds = lgb_model.predict(X_val)\n",
    "\n",
    "    # save out predictions\n",
    "    with open(preds_save_path, \"wb\") as f:\n",
    "        np.save(f, preds)\n",
    "    logger.success(f\"Predictions saved to {preds_save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python predict_gbm_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pth =DATA_DIR / \"val_preds.npy\"\n",
    "print(preds_pth)\n",
    "with open(preds_pth, \"rb\") as f:\n",
    "    val_preds = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(val_preds).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the validation part of the training data\n",
    "val_set = train_data[train_data.split == \"validation\"][\n",
    "    [\"uid\", \"region\", \"severity\"]\n",
    "].copy()\n",
    "val_set[\"pred\"] = val_preds\n",
    "\n",
    "val_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_scores = []\n",
    "for region in val_set.region.unique():\n",
    "    sub = val_set[val_set.region == region]\n",
    "    region_rmse = mean_squared_error(sub.severity, sub.pred, squared=False)\n",
    "    print(f\"RMSE for {region} (n={len(sub)}): {round(region_rmse, 4)}\")\n",
    "    region_scores.append(region_rmse)\n",
    "\n",
    "overall_rmse = np.mean(region_scores)\n",
    "print(f\"Final score: {overall_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's our RMSE across all validation data points?\n",
    "mean_squared_error(y_val, val_preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times did each severity level show up in our predictions vs. the actual values?\n",
    "val_results = pd.DataFrame({\"pred\": val_preds, \"actual\": y_val})\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        val_results.pred.value_counts().sort_index().rename(\"predicted\"),\n",
    "        val_results.actual.value_counts().sort_index().rename(\"actual\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ").rename_axis(\"severity_level_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image features for the test set\n",
    "test_features = submission_format.join(image_features, how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# make sure our features are in the same order as the submission format\n",
    "assert (test_features.index == submission_format.index).all()\n",
    "\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values\n",
    "for avg_col in [\"red_average\", \"green_average\", \"blue_average\"]:\n",
    "    test_features[avg_col] = test_features[avg_col].fillna(\n",
    "        test_features[avg_col].mean()\n",
    "    )\n",
    "for median_col in [\"red_median\", \"green_median\", \"blue_median\"]:\n",
    "    test_features[median_col] = test_features[median_col].fillna(\n",
    "        test_features[median_col].median()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature columns\n",
    "feature_cols = [\n",
    "    \"red_average\",\n",
    "    \"green_average\",\n",
    "    \"blue_average\",\n",
    "    \"red_median\",\n",
    "    \"green_median\",\n",
    "    \"blue_median\",\n",
    "]\n",
    "\n",
    "X_test = test_features[feature_cols].values\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out test features\n",
    "x_test_pth = BENCHMARK_DATA_DIR / \"x_test.npy\"\n",
    "with open(x_test_pth, \"wb\") as f:\n",
    "    np.save(f, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_pth = BENCHMARK_DATA_DIR / \"test_preds.npy\"\n",
    "%python predict_gbm_model.py --features-path {x_test_pth} --preds-save-path {test_preds_pth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our predictions\n",
    "with open(test_preds_pth, \"rb\") as f:\n",
    "    test_preds = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission_format.copy()\n",
    "submission[\"severity\"] = test_preds\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out our formatted submission\n",
    "submission_save_path = BENCHMARK_DATA_DIR / \"submission.csv\"\n",
    "submission.to_csv(submission_save_path, index=True)\n",
    "# make sure our saved csv looks correct\n",
    "%cat {submission_save_path} | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc681a220b04d10bf35e74116330022e2509b73a3f531ad28d5c26808ac7ca82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
